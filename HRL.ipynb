{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool-release Agent (PRA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRANetwork(nn.Module):\n",
    "    def __init__(self, input_dim=5, embedding_dim=128, hidden_dims=[256, 128], output_dim=2):\n",
    "        super(PRANetwork, self).__init__()\n",
    "        self.feature_extractor = nn.Linear(input_dim, embedding_dim)\n",
    "        layers = []\n",
    "        for i in range(len(hidden_dims)):\n",
    "            in_dim = embedding_dim if i == 0 else hidden_dims[i-1]\n",
    "            out_dim = hidden_dims[i]\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.ff_network = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.ff_network(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Route Planning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_layers, order_dim=5, delivery_dim=2, crowdsource_dim=5, inhouse_dim=3, ff_hidden_dim=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.order_embedding        = nn.Linear(order_dim, dim)                # order_dim = 5\n",
    "        self.crowdsource_embedding  = nn.Linear(crowdsource_dim, dim)    # crowdsource_dim = 5\n",
    "        self.inhouse_embedding      = nn.Linear(inhouse_dim, dim)            # inhouse_dim = 3\n",
    "        self.delivery_embedding     = nn.Linear(delivery_dim, dim)          \n",
    "        self.multi_head_attn_layers = nn.ModuleList([nn.MultiheadAttention(dim, num_heads, batch_first=True) for _ in range(num_layers)])\n",
    "        self.norm_layers            = nn.ModuleList([nn.BatchNorm1d(dim) for _ in range(num_layers)])\n",
    "        self.feed_forward           = nn.Sequential(\n",
    "                                        nn.Linear(dim, ff_hidden_dim),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(ff_hidden_dim, dim)\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x_o, x_d, x_c, x_i):\n",
    "        h_o = self.order_embedding(x_o)\n",
    "        h_d = self.delivery_embedding(x_d)\n",
    "        h_c = self.crowdsource_embedding(x_c)\n",
    "        h_i = self.inhouse_embedding(x_i)\n",
    "        \n",
    "        x = torch.cat((h_o, h_d, h_c, h_i), dim=0)\n",
    "        for multi_head_attn, norm in zip(self.multi_head_attn_layers, self.norm_layers):\n",
    "            attn_output, _ = multi_head_attn(x, x, x)\n",
    "            x = norm(x + attn_output)\n",
    "            x = norm(x + self.feed_forward(x))\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim, S, num_heads, is_cdrpa=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.multi_head_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.cdrpa_embedding = nn.Linear(dim*2+1+1, dim)    # W_Q'\n",
    "        self.cvrpa_embedding = nn.Linear(dim*2+1, dim)\n",
    "        self.is_cdrpa = is_cdrpa\n",
    "        self.W_Qs = nn.Linear(dim, dim)\n",
    "        self.W_Ks = nn.Linear(dim, dim)\n",
    "        self.S = S\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, context, H, visited_mask):\n",
    "        if self.is_cdrpa:\n",
    "            context = self.cdrpa_embedding(context)\n",
    "        else: \n",
    "            context = self.cvrpa_embedding(context)\n",
    "        \n",
    "        assert context.size(-1) == H.size(-1), 'Invalid shape'\n",
    "        \n",
    "        attn_mask = visited_mask.unsqueeze(0)\n",
    "        \n",
    "        H_c_t, _ = self.multi_head_attn(context, H, H, attn_mask=attn_mask)\n",
    "        \n",
    "        Qs = self.W_Qs(H_c_t)\n",
    "        Ks = self.W_Ks(H)\n",
    "\n",
    "        u = self.S * torch.tanh((Qs @ Ks.transpose(0, 1)) / torch.sqrt(torch.tensor(self.dim)))\n",
    "   \n",
    "        u = u.masked_fill(visited_mask == 1, float('-inf'))\n",
    "        \n",
    "        return F.softmax(u, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoutePolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutePolicyNetwork(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, num_layers=6, S=10):\n",
    "        super(RoutePolicyNetwork, self).__init__()\n",
    "        self.encoder = Encoder(dim, num_heads, num_layers)\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.S = S\n",
    "        \n",
    "    def forward(self, P_o, P_d, P_c, P_i, is_greedy=False):\n",
    "        P_c = sorted(P_c, key=lambda x: x['l_c'])           # Sorting crowdsource with latest depature time l_c\n",
    "        \n",
    "        x_o = torch.tensor([[*o['v_o'], o['e_o'], o['l_o'], o['w_o']] for o in P_o], dtype=torch.float32)\n",
    "        x_d = torch.tensor(P_d, dtype=torch.float32)\n",
    "        x_c = torch.tensor([[*c['u_c'], c['a_c'], c['l_c'], c['q_c']] for c in P_c], dtype=torch.float32)\n",
    "        x_i = torch.tensor([[*i['u_i'], i['q_i']] for i in P_i], dtype=torch.float32)\n",
    "        \n",
    "        print(x_o.shape)\n",
    "        print(x_d.shape)\n",
    "        print(x_c.shape)\n",
    "        print(x_i.shape)\n",
    "\n",
    "        assert x_o.shape[0] == x_d.shape[0], 'Invalid pair Order, Delivery'\n",
    "        \n",
    "        H_N = self.encoder(x_o, x_d, x_c, x_i)                   # Embedding graph -> forward pass\n",
    "        \n",
    "        num_nodes = H_N.size(0)\n",
    "        \n",
    "        h_o = H_N[:x_o.shape[0], :].clone()\n",
    "        h_d = H_N[x_o.shape[0]:x_o.shape[0]+x_d.shape[0], :].clone()\n",
    "        h_c = H_N[x_o.shape[0]+x_d.shape[0]:x_o.shape[0]+x_d.shape[0]+x_c.shape[0], :].clone()\n",
    "        h_i = H_N[x_o.shape[0]+x_d.shape[0]+x_c.shape[0]:, :].clone()\n",
    "        \n",
    "        assert h_o.shape[0] == x_o.shape[0], 'Invalid shape'\n",
    "        assert h_d.shape[0] == x_d.shape[0], 'Invalid shape'\n",
    "        assert h_c.shape[0] == x_c.shape[0], 'Invalid shape'\n",
    "        assert h_i.shape[0] == x_i.shape[0], 'Invalid shape'\n",
    "        \n",
    "        h_N_mean = torch.mean(H_N, dim=0)     # Global graph embedding\n",
    "        \n",
    "        P_o_d = P_o + P_d\n",
    "        h_o_d = H_N[:x_o.shape[0]+x_d.shape[0], :].clone()\n",
    "        \n",
    "        # print(H_N.shape)\n",
    "        # print(h_N_mean.shape)\n",
    "        # print(f\"Total Orders: {x_o.size(0)}\")\n",
    "        # print(f\"Total Crowdsources: {x_c.size(0)}\")\n",
    "        # print(f\"Total Inhouses: {x_i.size(0)}\")\n",
    "        \n",
    "        # output_dim must be the number of the order nodes\n",
    "        decoder_cdrpa = Decoder(dim=self.dim, S=self.S, num_heads=self.num_heads, is_cdrpa=True)\n",
    "        decoder_cvrpa = Decoder(dim=self.dim, S=self.S, num_heads=self.num_heads)\n",
    "\n",
    "        S_CDPRA_t = copy.deepcopy(P_o)                  # State S_CDPRA_t = P_o -> node of orders\n",
    "        # print(f\"P_o nodes: {S_CDPRA_t}\")\n",
    "        print(f'H_N: {H_N.shape}')\n",
    "        \n",
    "        visited_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        \n",
    "        visited_mask[x_o.shape[0]+x_d.shape[0]:] = 1\n",
    "        \n",
    "        visited_idx_node = []                           # Visited index of node in P_o + P_d\n",
    "        routes_policy_c = []                            # Routes policy for c\n",
    "        log_probs = []                                  # Log probabilities of the selected nodes\n",
    "        \n",
    "        for idx_c in range(x_c.size(0)):                # CDRPA\n",
    "            if len(visited_idx_node) >= len(P_o + P_d): \n",
    "                break\n",
    "            \n",
    "            t = 1\n",
    "            q = 0\n",
    "            t_travel = 0                                # Total time travel\n",
    "            h_uc = h_c[idx_c]                           # t = 1, the driver’s location information is the destination’s embedded feature h_uc\n",
    "            c = P_c[idx_c]                              # Crowdsource's information\n",
    "            Q = c['q_c']                                # Limitation capacity of crowdsource\n",
    "            L = c['l_c'] - c['a_c']                     # Limitation total time travel (driver's point -> pick-up -> next pick-up)\n",
    "            H_c_t_prev = None                           # Previous embedding previous visited node \n",
    "            v_o_prev = []                               # Previous v_o node\n",
    "            route = []\n",
    "            pi_c = []                                   # Visited node index for c\n",
    "            q_res = 0\n",
    "            t_travel_res = 0\n",
    "            \n",
    "            while q < Q and t_travel < L:\n",
    "                if len(visited_idx_node) >= len(P_o + P_d): \n",
    "                    break\n",
    "                \n",
    "                q_r = torch.tensor(Q - q, dtype=torch.float32).unsqueeze(0)\n",
    "                t_travel_r = torch.tensor(L - t_travel, dtype=torch.float32).unsqueeze(0)\n",
    "                H_c_t = torch.cat((h_N_mean, h_uc, q_r, t_travel_r), dim=-1) if t == 1 else torch.cat((h_N_mean, H_c_t_prev, q_r, t_travel_r), dim=-1)\n",
    "\n",
    "                probs_node = decoder_cdrpa(H_c_t.unsqueeze(0), H_N, visited_mask)   # Probability distribution over node of Orders\n",
    "                \n",
    "                node = None\n",
    "                if is_greedy:\n",
    "                    _, node = torch.max(probs_node, dim=-1)\n",
    "                else:\n",
    "                    node = torch.multinomial(probs_node, 1)                         # Order node selection using sampling method\n",
    "                \n",
    "                p_node = probs_node.squeeze(0)[node.item()]\n",
    "                log_prob = torch.log(p_node)                                        # Log probability of the selected node\n",
    "                # print(f\"node: {node.item()}\")\n",
    "                \n",
    "                assert node.item() < x_o.shape[0]+x_d.shape[0], 'Out of range selection'\n",
    "                \n",
    "                o = P_o_d[node.item()]\n",
    "                \n",
    "                if isinstance(o, dict) and (q + o['w_o']) > Q:                                \n",
    "                    q_res = q                                                       # Over-capacity\n",
    "                \n",
    "                if isinstance(o, dict):\n",
    "                    q += o['w_o']                                                   # Consider capacity\n",
    "                \n",
    "                if t == 1:\n",
    "                    t_travel += self.t_uo(c['u_c'], o['v_o']) if isinstance(o, dict) else self.t_uo(c['u_c'], o)\n",
    "                    t_travel_res += t_travel\n",
    "                elif isinstance(o, dict) and (t_travel + self.t_uo(v_o_prev, o['v_o'])) > L:\n",
    "                    t_travel_res = t_travel                                         # Over-time travel\n",
    "                    t_travel += self.t_uo(v_o_prev, o['v_o']) if isinstance(o, dict) else self.t_uo(v_o_prev, o)\n",
    "                else:\n",
    "                    t_travel += self.t_uo(v_o_prev, o['v_o']) if isinstance(o, dict) else self.t_uo(v_o_prev, o)    # Time travel from prev v_o to next v_o\n",
    "                \n",
    "                route += [o] if q < Q and t_travel < L else []\n",
    "                pi_c += [node.item()] if q < Q and t_travel < L else []\n",
    "                log_probs += [log_prob.item()] if q < Q and t_travel < L else []\n",
    "                \n",
    "                if q < Q and t_travel < L:\n",
    "                    visited_mask[node.item()] = 1                                       # Mask visited node\n",
    "                    visited_idx_node.append(node.item())                            # Keep track visited node\n",
    "                \n",
    "                v_o_prev = o['v_o'] if isinstance(o, dict) else o\n",
    "                H_c_t_prev = h_o_d[node.item()]                                     # Embedded feature of the driver’s previous visited node\n",
    "                    \n",
    "                t += 1\n",
    "            \n",
    "            # print(\"--------------------------------------------\")\n",
    "            # print(f\"Driver {c}\")\n",
    "            # print(f\"Total capacity: {q_res}\")\n",
    "            # print(f\"Total travel time: {t_travel_res}\")\n",
    "            # print(f\"Route: {route}\")\n",
    "            # print(f\"Visited idx node c: {visited_idx_node_c}\")\n",
    "            # print(f\"Visited P_o: {visited_idx_node_Po}\")\n",
    "            # print(f\"Visited len(P_o): {len(visited_idx_node_Po)}\")\n",
    "            routes_policy_c.append({\n",
    "                \"crowdsource\": c,\n",
    "                \"pi_c\": pi_c,\n",
    "                \"total_capacity\": q_res,\n",
    "                \"total_travel_time\": t_travel_res,\n",
    "                \"route\": route,\n",
    "                # \"visited_idx_node\": visited_idx_node,\n",
    "            })\n",
    "        \n",
    "        routes_policy_i = []                    # Routes policy for i\n",
    "        for idx_i in range(x_i.size(0)):        # CVRPA\n",
    "            if len(visited_idx_node) >= len(P_o + P_d): \n",
    "                break\n",
    "            \n",
    "            t = 1\n",
    "            q = 0\n",
    "            h_ui = h_i[idx_i]                   # t = 1, the driver’s location information is the destination’s embedded feature h_uc\n",
    "            i = P_i[idx_i]                      # Crowdsource's information\n",
    "            Q = i['q_i']                        # Limitation capacity of inhouse delivery\n",
    "            H_k_t_prev = None                   # Previous embedding previous visited node \n",
    "            route = []\n",
    "            pi_i = []             # Visited node index for c\n",
    "            q_res = 0\n",
    "            while q < Q:\n",
    "                if len(visited_idx_node) >= len(P_o + P_d): \n",
    "                    break\n",
    "                \n",
    "                q_r = torch.tensor(Q - q, dtype=torch.float32).unsqueeze(0)\n",
    "                H_k_t = torch.cat((h_N_mean, h_ui, q_r), dim=-1) if t == 1 else torch.cat((h_N_mean, H_k_t_prev, q_r), dim=-1)\n",
    "                \n",
    "                probs_node = decoder_cvrpa(H_k_t.unsqueeze(0), H_N, visited_mask)       # Probability distribution over node of orders\n",
    "                \n",
    "                node = None\n",
    "                if is_greedy:\n",
    "                    _, node = torch.max(probs_node, dim=-1)\n",
    "                else:\n",
    "                    node = torch.multinomial(probs_node, 1)                     # Order node selection using sampling method\n",
    "                \n",
    "                p_node = probs_node.squeeze(0)[node.item()]\n",
    "                log_prob = torch.log(p_node)                                    # Log probability of the selected node\n",
    "                \n",
    "                assert node.item() < x_o.shape[0]+x_d.shape[0], 'Out of range selection'\n",
    "                \n",
    "                o = P_o_d[node.item()]\n",
    "                \n",
    "                if isinstance(o, dict) and (q + o['w_o']) > Q:                                \n",
    "                    q_res = q                                                   # Over-capacity\n",
    "                  \n",
    "                if isinstance(o, dict):          \n",
    "                    q += o['w_o']                                               # Consider capacity\n",
    "                \n",
    "                route += [o] if q < Q else []\n",
    "                pi_i += [node.item()] if q < Q else []\n",
    "                log_probs += [log_prob.item()] if q < Q else []\n",
    "                \n",
    "                if q < Q:\n",
    "                    visited_mask[node.item()] = 1\n",
    "                    visited_idx_node.append(node.item())\n",
    "                    \n",
    "                H_k_t_prev = h_o_d[node.item()]                                   # Embedded feature of the driver’s previous visited node\n",
    "                    \n",
    "                t += 1\n",
    "            \n",
    "            # print(\"--------------------------------------------\")\n",
    "            # print(f\"In-house delivery {i}\")\n",
    "            # print(f\"Total capacity: {q_res}\")\n",
    "            # print(f\"Route: {route}\")\n",
    "            # print(f\"Visited idx node i: {visited_idx_node_i}\")\n",
    "            # print(f\"Visited P_o: {visited_idx_node_Po}\")\n",
    "            # print(f\"Visited len(P_o): {len(visited_idx_node_Po)}\")\n",
    "            routes_policy_i.append({\n",
    "                \"inhouse\": i,\n",
    "                \"pi_i\": pi_i,\n",
    "                \"total_capacity\": q_res,\n",
    "                \"route\": route,\n",
    "                # \"visited_idx_node\": visited_idx_node,\n",
    "            })\n",
    "            \n",
    "        return routes_policy_c, routes_policy_i, log_probs\n",
    "    \n",
    "    def t_uo(self, u_c, v_o):\n",
    "        speed = 30\n",
    "        return np.around(np.sqrt((u_c[0] - v_o[0])**2 + (u_c[1] - v_o[1])**2) / speed, 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 3\n",
    "grid_spacing = 0.25\n",
    "num_grid_points = int(grid_size / grid_spacing)\n",
    "grid_points = [[x, y] for x in range(num_grid_points) for y in range(num_grid_points)]\n",
    "\n",
    "def generate_coordinates(num, initial=False):\n",
    "    if initial:\n",
    "        num_zero_one = int(num * 0.85)\n",
    "        num_other = num - num_zero_one\n",
    "\n",
    "        depot_coords = [[0, 1]] * num_zero_one\n",
    "\n",
    "        other_coords = random.sample(grid_points, num_other)\n",
    "\n",
    "        sample = depot_coords + other_coords\n",
    "        random.shuffle(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    return random.sample(grid_points, num)\n",
    "\n",
    "print(generate_coordinates(5, initial=True))\n",
    "print(generate_coordinates(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random orders pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_orders_pool(num_orders, time_step, len_time_step=10, initial=False):\n",
    "    \"\"\"\n",
    "    time_step: 0, 1, 2,..., 54\n",
    "    len_time_step: length of a period = 10\n",
    "    e_o: early delivery time\n",
    "    l_o: late delivery time\n",
    "    v_o: co-ordinates pick up order\n",
    "    d_o: co-ordinates delivery order\n",
    "    a_o: order arrived time\n",
    "    p_o: pick up time\n",
    "    w_o: weight order\n",
    "    tw_o: time window between pick up and late delivery time\n",
    "    \"\"\"\n",
    "    a_o = np.around(\n",
    "        np.around(np.random.uniform(0, len_time_step, num_orders), 2)\n",
    "        + time_step * len_time_step, 2\n",
    "    )\n",
    "    p_o = np.around(a_o + np.around(np.random.uniform(0, 45), 2), 2)\n",
    "    tw_o = np.around(np.random.uniform(60, 120), 2)\n",
    "    e_o = np.around(a_o + p_o + tw_o, 2)\n",
    "    l_o = np.around(p_o + tw_o, 2)\n",
    "    w_o = np.around(np.random.uniform(0.5, 10, num_orders), 2)\n",
    "    v_o = generate_coordinates(num_orders, initial=initial)\n",
    "    d_o = generate_coordinates(num_orders)\n",
    "\n",
    "    return [{\"e_o\": i, \"l_o\": j, \"v_o\": k, \"w_o\": m} for i, j, k, m in zip(e_o, l_o, v_o, w_o)], d_o\n",
    "\n",
    "\n",
    "pool_orders = generate_random_orders_pool(10, 0, 10, initial=True)\n",
    "pool_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_up_orders = pool_orders[0] \n",
    "x_o = torch.tensor([[*o['v_o'], o['e_o'], o['l_o'], o['w_o']] for o in pick_up_orders], dtype=torch.long)\n",
    "x_o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random crowdsources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_crowdsources_pool(\n",
    "    num_crowdsources, time_step, len_time_step=10, initial=False\n",
    "):\n",
    "    \"\"\"\n",
    "    time_step: 0, 1, 2,..., 54\n",
    "    len_time_step: length of a period = 10\n",
    "    a_c: arrive time\n",
    "    l_c: leave time\n",
    "    q_c: carrying capacity\n",
    "    u_c: co-ordinates crowdsource\n",
    "    \"\"\"\n",
    "    a_c = np.around(\n",
    "        np.around(np.random.uniform(0, len_time_step, num_crowdsources), 2)\n",
    "        + time_step * len_time_step, 2\n",
    "    )\n",
    "    l_c = np.around(a_c + np.around(np.random.uniform(120, 180), 2), 2)\n",
    "    q_c = np.around(np.random.uniform(5, 20, num_crowdsources), 2)\n",
    "    u_c = generate_coordinates(num_crowdsources)\n",
    "\n",
    "    return [{\"a_c\": i, \"l_c\": j, \"q_c\": k, \"u_c\": m} for i, j, k, m in zip(a_c, l_c, q_c, u_c)]\n",
    "\n",
    "\n",
    "c = generate_random_crowdsources_pool(10, 0, 10, initial=True)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = sorted(c, key=lambda x: x['l_c'])\n",
    "sorted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random in-house delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_inhouse_pool(\n",
    "    num_trucks, num_motorbikes\n",
    "):\n",
    "    return [{\"q_i\": 1000, \"u_i\": [0,1]}]*num_trucks + [{\"q_i\": 25, \"u_i\": [0,1]}]*num_motorbikes\n",
    "\n",
    "\n",
    "generate_random_inhouse_pool(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = 30\n",
    "\n",
    "def t_oc(u_c, v_o):\n",
    "    return np.around(np.sqrt((u_c[0] - v_o[0])**2 + (u_c[1] - v_o[1])**2) / speed, 2)\n",
    "\n",
    "def t_ave(P_o_tau, P_c_tau):\n",
    "    return sum(t_oc(u_c=c['u_c'], v_o=o['v_o']) for o in P_o_tau for c in P_c_tau) / len(P_c_tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward at each node selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rewards_at_each_node(routes_policy_c, \n",
    "                                   routes_policy_i, \n",
    "                                   t_oc, \n",
    "                                   truck_cost,\n",
    "                                   motobike_cost,\n",
    "                                   late_penalty, \n",
    "                                   duplicate_penalty, \n",
    "                                   crowdsources_cost):\n",
    "    \n",
    "    waiting_cost_c = crowdsources_cost*1.5\n",
    "    # rewards_per_route_c = []  # List to store rewards for each route\n",
    "    rewards = []\n",
    "    for index, c in enumerate(routes_policy_c):\n",
    "        route = c['route']\n",
    "        if not route:\n",
    "            # rewards_per_route_c.append([])\n",
    "            rewards.extend([])\n",
    "            continue\n",
    "\n",
    "        # route_rewards = []  # Rewards for the current route\n",
    "        T_pi_c = t_oc(c['crowdsource']['u_c'], route[0]['v_o'] if isinstance(route[0], dict) else route)  # Initial travel time\n",
    "        initial_waiting_cost = max(route[0]['e_o'] - T_pi_c, 0) * waiting_cost_c\n",
    "        T_pi_c += t_oc(route[0]['v_o'], route[0]['d_o'])\n",
    "        late_delivery_penalty = late_penalty if T_pi_c > route[0]['l_o'] else 0\n",
    "        duplicate_penalty = 0  # Check for duplicate visit\n",
    "        initial_reward = -(crowdsources_cost * T_pi_c + initial_waiting_cost + late_delivery_penalty + duplicate_penalty)\n",
    "        # route_rewards.append(initial_reward)\n",
    "        rewards.append(initial_reward)\n",
    "        # return\n",
    "        for n in range(len(route) - 1):\n",
    "            # Travel from current delivery to next pickup\n",
    "            T_pi_c += t_oc(route[n]['d_o'], route[n+1]['v_o'])\n",
    "            duplicate_penalty = duplicate_penalty if c['visited_idx_node'][n+1] in c['visited_idx_node'] else 0\n",
    "            waiting_time_cost = max(route[n+1]['e_o'] - T_pi_c, 0) * waiting_cost_c\n",
    "            T_pi_c += t_oc(route[n+1]['v_o'], route[n+1]['d_o'])\n",
    "            late_delivery_penalty = late_penalty if T_pi_c > route[n+1]['l_o'] else 0\n",
    "\n",
    "            node_reward = -(crowdsources_cost * T_pi_c + waiting_time_cost + late_delivery_penalty)\n",
    "            # route_rewards.append(node_reward)\n",
    "            rewards.append(node_reward)\n",
    "\n",
    "        # rewards_per_route_c.append(route_rewards)\n",
    "        \n",
    "    # Reward for route i\n",
    "    rewards_per_route_i = []  # List to store rewards for each route for in-house drivers\n",
    "\n",
    "    for index, i in enumerate(routes_policy_i):\n",
    "        route = i['route']\n",
    "        if not route:\n",
    "            # rewards_per_route_i.append([])\n",
    "            rewards.extend([])\n",
    "            continue\n",
    "\n",
    "        route_rewards = []  # Rewards for the current route\n",
    "        waiting_cost_i = 1.5*truck_cost if i['inhouse'] == 1000 else 1.5*motobike_cost\n",
    "        T_pi_i = t_oc(i['inhouse']['u_i'], route[0]['v_o'])\n",
    "        initial_waiting_cost = max(route[0]['e_o'] - T_pi_i, 0) * waiting_cost_i\n",
    "        T_pi_i += t_oc(route[0]['v_o'], route[0]['d_o'])\n",
    "        late_delivery_penalty = late_penalty if T_pi_i > route[0]['l_o'] else 0\n",
    "        duplicate_penalty =  0\n",
    "        inhouse_cost = 1\n",
    "        if i['inhouse'] == 1000:\n",
    "            inhouse_cost = truck_cost\n",
    "        else:\n",
    "            inhouse_cost = motobike_cost\n",
    "            \n",
    "        initial_reward = -(inhouse_cost * T_pi_i + initial_waiting_cost + late_delivery_penalty + duplicate_penalty)\n",
    "        # route_rewards.append(initial_reward)\n",
    "        rewards.append(initial_reward)\n",
    "\n",
    "        for n in range(len(route) - 1):\n",
    "            T_pi_i += t_oc(route[n]['d_o'], route[n+1]['v_o'])\n",
    "            duplicate_penalty = duplicate_penalty if i['visited_idx_node'][n+1] in i['visited_idx_node'] else 0\n",
    "            waiting_time_cost = max(route[n+1]['e_o'] - T_pi_i, 0) * waiting_cost_i\n",
    "            T_pi_i += t_oc(route[n+1]['v_o'], route[n+1]['d_o'])\n",
    "            late_delivery_penalty = late_penalty if T_pi_i > route[n+1]['l_o'] else 0\n",
    "\n",
    "            node_reward = -(inhouse_cost * T_pi_i + waiting_time_cost + late_delivery_penalty)\n",
    "            # route_rewards.append(node_reward)\n",
    "            rewards.append(node_reward)\n",
    "\n",
    "        # rewards_per_route_i.append(route_rewards)\n",
    "\n",
    "    # return rewards_per_route_c, rewards_per_route_i \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(num_episodes, \n",
    "                  num_batches=1, \n",
    "                  planning_horizons=54, \n",
    "                  len_periods=10, \n",
    "                  num_trucks=10, \n",
    "                  num_motobikes=20,\n",
    "                  truck_cost=5,\n",
    "                  motobike_cost=3,\n",
    "                  crowdsources_cost=3,\n",
    "                  late_penalty=15,\n",
    "                  duplicate_penalty=10\n",
    "                  ):\n",
    "    \n",
    "    policy_network = RoutePolicyNetwork(dim=128, S=10, num_heads=8, num_layers=6)\n",
    "    pra_network = PRANetwork()\n",
    "    policy_optimizer = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "    pra_optimizer = optim.Adam(pra_network.parameters(), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(num_episodes):\n",
    "        for batch in range(num_batches):\n",
    "            n_io = np.round(np.random.uniform(90, 120)).astype(int)     # Number of initial orders\n",
    "            n_ic = np.round(np.random.uniform(50, 60)).astype(int)      # Number of initial crowdsources\n",
    "            P_o = []                                                    # Orders pool\n",
    "            P_d = []                                                    # Delivery pool (pair with Order's pool)\n",
    "            P_c = []                                                    # Crowdsources pool\n",
    "            T = len_periods                                             # 8:10\n",
    "            \n",
    "            P_i = generate_random_inhouse_pool(num_trucks, num_motobikes)   # In-house delivery pool\n",
    "            \n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            for tau in range(planning_horizons):                        # Planning horizons -> #time steps which decision-making process occurs\n",
    "                N_o = np.round(np.random.uniform(30, 45)).astype(int)   # Sample new placed orders\n",
    "                N_c = np.round(np.random.uniform(15, 25)).astype(int)   # Sample new arrived crowdsources\n",
    "                \n",
    "                P_o_tau = []\n",
    "                P_d_tau = []\n",
    "                P_c_tau = []\n",
    "                \n",
    "                if tau == 0:\n",
    "                    init_o, d_io = generate_random_orders_pool(n_io, 0, len_periods, initial=True)\n",
    "                    rand_o, d_ro = generate_random_orders_pool(N_o, tau, len_periods)\n",
    "                    P_o_tau = init_o + rand_o\n",
    "                    P_d_tau = d_io + d_ro\n",
    "                    P_c_tau = generate_random_crowdsources_pool(n_ic, 0, len_periods, initial=True) + generate_random_crowdsources_pool(N_c, tau, len_periods)\n",
    "                \n",
    "                else:\n",
    "                    rand_o, d_ro = generate_random_orders_pool(N_o, tau, len_periods)\n",
    "                    P_o_tau = P_o + rand_o\n",
    "                    P_d_tau = P_d + d_ro\n",
    "                    P_c_tau = P_c + generate_random_crowdsources_pool(N_c, tau, len_periods)\n",
    "                \n",
    "                # State S_PRA_tau\n",
    "                n_tau = len(P_o_tau)                                        # Number of current orders\n",
    "                m_tau = len(P_c_tau)                                        # Number of current crowdsources\n",
    "                l_star_o_tau = min(P_o_tau, key=lambda x: x['l_o'])['l_o']  # Late delivery time of the most urgent orders\n",
    "                l_star_c_tau = min(P_c_tau, key=lambda x: x['l_c'])['l_c']  # Last departure time of almost leaving crowdsource\n",
    "                t_ave_tau = t_ave(P_o_tau, P_c_tau)                         # Average travel time between locations of crowdsources and pick-up orders\n",
    "                S_PRA_tau = [n_tau, m_tau, l_star_o_tau, l_star_c_tau, t_ave_tau]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.tensor(S_PRA_tau, dtype=torch.float).unsqueeze(0)\n",
    "                    probabilities = pra_network(state_tensor)\n",
    "                    action = torch.multinomial(probabilities, 1)            # The released decision uses the sampling method\n",
    "                    \n",
    "                if action.item() == 1:\n",
    "                    routes_policy_c, routes_policy_i, log_probs_tau = policy_network(P_o_tau, P_d_tau, P_c_tau, P_i)   # Lower-agent construct route policy\n",
    "                    route_rewards = calculate_rewards_at_each_node(routes_policy_c, routes_policy_i, t_oc, truck_cost, motobike_cost, late_penalty, duplicate_penalty, crowdsources_cost)\n",
    "                    \n",
    "                    assert len(route_rewards) == len(log_probs_tau), 'Invalid rewards, log_probs'\n",
    "                    \n",
    "                    log_probs.extend(log_probs_tau)\n",
    "                    rewards.extend(route_rewards)\n",
    "                    print(f\"Epoch: {epoch}, Tau: {tau}\")\n",
    "                    P_o_tau = []\n",
    "                    P_c_tau = []\n",
    "                    P_d_tau = []\n",
    "                \n",
    "                P_o.extend(P_o_tau)\n",
    "                P_d.extend(P_d_tau)\n",
    "                T += len_periods                                            # Next length period 8:20, crowdsource 8:15 -> out-dated\n",
    "                P_c_tau = [c for c in P_c_tau if c['l_c'] > T]              # Weed out expired crowd drivers\n",
    "                P_c.extend(P_c_tau)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            rewards = torch.tensor(rewards)\n",
    "            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-9)   # Normalize rewards\n",
    "            policy_loss = []\n",
    "            for log_prob, R in zip(log_probs, rewards):\n",
    "                policy_loss.append(-log_prob * R)                           # Negative log probability multiplied by return\n",
    "            \n",
    "            policy_loss = torch.sum(torch.stack(policy_loss))\n",
    "\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "            pra_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            pra_optimizer.step()\n",
    "            \n",
    "            print(f\"Epoch: {epoch}, Total Reward: {sum(rewards)}, Loss: {policy_loss}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mask = [1, 5]\n",
    "num_nodes = 10\n",
    "visited_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "visited_mask[[0,2]] = 1\n",
    "visited_mask[7:] = 1\n",
    "visited_mask, visited_mask.unsqueeze(0).repeat(num_nodes, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(5, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
